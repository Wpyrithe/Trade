import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Génération de données avec transformations supplémentaires et caractéristiques

def generate_data(n_samples=1000, seq_length=15, noise_level=0.5, end_variation=0.3, random_noise=0.2, initial_noise=1.0): 
    X, y = [], []
    for _ in range(n_samples):
        rand_choice = np.random.rand()
        if rand_choice < 0.3:  # Séquence globalement croissante
            seq = np.sort(np.random.randn(seq_length) + np.random.rand())
            seq += np.random.uniform(-initial_noise, initial_noise, seq_length)  # Bruit initial
            noise = np.random.uniform(-noise_level, noise_level, seq_length)
            seq += noise  # Ajout de bruit
            seq[-3:] += np.random.uniform(-end_variation, end_variation, 3)  # Plus de variations sur la fin
            label = 1
        elif rand_choice < 0.6:  # Séquence globalement décroissante
            seq = -np.sort(np.random.randn(seq_length) + np.random.rand())
            seq += np.random.uniform(-initial_noise, initial_noise, seq_length)  # Bruit initial
            noise = np.random.uniform(-noise_level, noise_level, seq_length)
            seq += noise  # Ajout de bruit
            seq[-3:] += np.random.uniform(-end_variation, end_variation, 3)  # Plus de variations sur la fin
            label = 0
            
        else:  # Séquence aléatoire
            seq = np.random.randn(seq_length) * 2  # Plus de dispersion
            seq += np.random.uniform(-random_noise, random_noise, seq_length)  # Ajout d'un léger bruit
            label = 2
            
        # Normalisation entre 0 et 1
        seq = (seq - np.min(seq)) / (np.max(seq) - np.min(seq) + 1e-6)  # Éviter division par zéro
        #print(seq) comportement normal/attendu

        # Ajouter des caractéristiques supplémentaires
        diff_seq = np.diff(seq, prepend=seq[0])  # Différences successives
        mean_diff = np.mean(diff_seq)  # Moyenne des différences
        max_min_ratio = (np.max(seq) - np.min(seq)) / (max(np.abs(np.min(seq)), 1e-3))  # Évite les explosions
        #minmaxratio = 10000... 
        features = np.concatenate([seq, diff_seq, [mean_diff, max_min_ratio]])
        #print(seq, diff_seq, mean_diff, max_min_ratio)
        #print(features)
        X.append(features)
        y.append(label)
    # print de la seq et voir les valeurs. puis print de X et comparer au print de Xf dans le code de trade
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)

# Charger les données
X, y = generate_data()
X_train, y_train = torch.tensor(X), torch.tensor(y)
print(X_train.shape, X_train.dtype)

# Définition du modèle amélioré
class TrendPredictor(nn.Module):
    def __init__(self, input_size):
        super(TrendPredictor, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.bn1 = nn.BatchNorm1d(64)
        self.fc2 = nn.Linear(64, 32)
        self.bn2 = nn.BatchNorm1d(32)
        self.fc3 = nn.Linear(32, 3)  # 3 classes : décroissant, croissant, aléatoire
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x

# Initialisation du modèle
input_size = X.shape[1]  # Mise à jour de la taille d'entrée
#print(X.shape[1])
model = TrendPredictor(input_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

# Entraînement du modèle avec régularisation
T = 1
while T>0.90:
    epochs = 150
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        #if epoch % 10 == 0:
        #    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

    # Tester sur une nouvelle séquence
    X_test, y_test = generate_data(n_samples=1000)
    #print(X_test[0], X_test[1], X_test[2], X_test[3])
    X_test_tensor = torch.tensor(X_test)
    #print(X_test_tensor, X_test_tensor.size())
    predictions = model(X_test_tensor).argmax(dim=1).numpy()
    #print("Prédictions :", predictions)
    #print("Vraies valeurs :", y_test)

    h=0
    for i in range(len(predictions)):
        if predictions[i] != y_test[i]:
            h = h+1
    T = h/1000
    print("nombre d'erreurs : ", h)
    print("taux d'erreur: ", T)
    print("Input shape:", X_train.shape)
    print("First input sample:", X_train[0])
    #print(model.fc1.weight[:5])
    #print(model(X_train))
#print(predictions)

'''
print("Model's state_dict:")
for param_tensor in model.state_dict():
    print(param_tensor, "\t", model.state_dict()[param_tensor].size())

# Print optimizer's state_dict
print("Optimizer's state_dict:")
for var_name in optimizer.state_dict():
    print(var_name, "\t", optimizer.state_dict()[var_name])
'''
torch.save(model.state_dict(), "ModelTrade.pt")





